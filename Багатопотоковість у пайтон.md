### Вступ: Концептуальні основи багатопотоковості

Перш ніж занурюватися в технічні деталі реалізації багатопотоковості в Python, важливо зрозуміти, які фундаментальні проблеми вона вирішує і як еволюціонували підходи до паралельного виконання задач. Цей вступ закладе концептуальну основу, яка допоможе усвідомити, _чому_ існують такі інструменти, як потоки, процеси та механізми синхронізації.

### 1. Еволюція обчислень: від однієї задачі до процесів

На зорі комп'ютерної ери все було просто. Комп'ютер складався з процесора та пам'яті, а операційна система могла виконувати лише **одну програму в один момент часу**. Якщо вам потрібно було запустити текстовий редактор, а потім калькулятор, ви мали спочатку закрити редактор, і лише після цього запускати калькулятор. Це було вкрай незручно.

#### Народження багатозадачності

З часом користувачі захотіли запускати кілька програм одночасно. Щоб задовольнити цю потребу, операційні системи ввели поняття **процесу**. Процес — це, по суті, екземпляр програми, що виконується, зі своїм власним ізольованим простором у пам'яті.

Першою спробою реалізувати багатозадачність була **кооперативна багатозадачність**. Ідея полягала в тому, що кожна програма мала "по-джентльменськи" час від часу добровільно віддавати керування процесором операційній системі, щоб та могла запустити іншу програму.

**Проблеми кооперативного підходу:**
- **Зависання системи:** Якщо одна програма через помилку або навмисно не віддавала керування (наприклад, зациклювалася), вся система "зависала", оскільки інші програми не могли отримати доступ до процесора.
- **Пошкодження даних:** Програми працювали в спільному просторі пам'яті. Одна програма могла випадково або навмисно записати дані в область пам'яті, що належала іншій програмі, спричиняючи збої та непередбачувану поведінку.

#### Рішення: витискаюча багатозадачність та ізоляція

Ці проблеми були настільки серйозними, що вимагали кардинального рішення на рівні "заліза". Сучасні процесори отримали апаратну підтримку для створення **ізольованих процесів**.  
Ключові інновації:  
1. **Віртуальна пам'ять:** Кожен процес отримує свій власний віртуальний адресний простір. Він "думає", що має у своєму розпорядженні всю пам'ять комп'ютера, хоча насправді операційна система відображає цю віртуальну пам'ять на реальні фізичні комірки. Це унеможливлює пряме втручання одного процесу в пам'ять іншого.  
2. **Витискаюча багатозадачність (Preemptive Multitasking):** Тепер операційна система, а не програма, повністю контролює розподіл процесорного часу. За допомогою апаратного таймера вона може в будь-який момент перервати виконання одного процесу і передати керування іншому. Це гарантує, що жодна програма не зможе монополізувати процесор і "підвісити" систему.

Цей підхід вирішив проблему стабільності та безпеки, дозволивши користувачам комфортно працювати з кількома програмами одночасно. Однак незабаром з'явилася нова проблема, яка вимагала ще більш витонченого рішення.

### 2. Проблема блокуючих операцій та народження потоків

Ізольовані процеси чудово вирішили проблему стабільності, але вони породили нову складність, особливо помітну в програмах з графічним інтерфейсом. Проблема полягає у **блокуючих операціях**.

**Блокуюча операція** — це будь-яка дія, що змушує програму "заснути" і чекати на завершення зовнішньої події. Класичні приклади:  
- Читання великого файлу з диска.  
- Запит до бази даних.  
- Завантаження веб-сторінки з мережі.  
- Очікування на введення даних користувачем.

Уявіть собі настільну програму, яка працює в одному процесі. Користувач натискає кнопку "Зберегти", і програма починає записувати великий файл на диск. Поки триває ця операція, весь процес "заблокований". Він не може реагувати на інші дії користувача: інтерфейс не оновлюється, кнопки не натискаються, вікно не можна пересунути. Для користувача програма просто "зависла".

#### Перші спроби вирішення: асинхронність та нові процеси

Програмісти намагалися боротися з цим різними способами. Один із них — **асинхронні виклики**. Це спеціальний режим, у якому програма може сказати операційній системі: "Почни, будь ласка, цю операцію, а коли закінчиш — повідом мене". Це дозволяло програмі продовжувати працювати, не чекаючи завершення, але такий підхід був дуже складним у реалізації та вимагав значних зусиль від програміста.

Іншим рішенням було породження нового **процесу** для кожної довготривалої задачі. Наприклад, для збереження файлу можна було запустити окремий фоновий процес. Це працювало, але було неефективно:  
- **Ресурсоємність:** Створення нового процесу — це "важка" операція, що вимагає від операційної системи виділення пам'яті та значних обчислювальних ресурсів.  
- **Складна комунікація:** Оскільки процеси ізольовані, організувати обмін даними між ними (наприклад, щоб фоновий процес повідомив основний про прогрес збереження) було складно.

#### Елегантне рішення: Потоки (Threads)

Саме тут на сцену виходять **потоки**. Потік — це, по суті, "легкий процес" всередині основного процесу. Це окрема послідовність інструкцій, яку операційна система може виконувати незалежно.

Ключова відмінність потоку від процесу:  
- **Спільна пам'ять:** Усі потоки одного процесу працюють у спільному адресному просторі. Це означає, що вони мають прямий доступ до одних і тих самих змінних та об'єктів.  
- **Легкість:** Створення та перемикання між потоками набагато "дешевше" для операційної системи, ніж для процесів.

Завдяки потокам, можна було винести блокуючу операцію (збереження файлу, завантаження з мережі) в окремий фоновий потік. Основний потік при цьому залишався вільним і продовжував реагувати на дії користувача, забезпечуючи плавний та чутливий інтерфейс.

Навіть на одноядерному процесорі це давало величезний виграш у зручності. Поки один потік чекав на відповідь від мережі (і не навантажував процесор), операційна система могла передати керування іншому потоку, який оновлював інтерфейс.

Потоки здавалися ідеальним рішенням. Вони вирішили проблему "зависання" програм і значно спростили взаємодію між паралельними задачами. Однак спільна пам'ять, яка була їхньою найбільшою перевагою, виявилася і їхнім найбільшим прокляттям, породивши цілий клас нових, складних помилок.

### 3. У чому насправді полягає "проблема багатопотоковості"

Як ми з'ясували, головна перевага потоків — спільна пам'ять — виявилася і їхньою найбільшою проблемою. Коли кілька потоків одночасно читають і змінюють одні й ті самі дані, виникає хаос. Але в чому саме полягає корінь цього хаосу?

#### Поширена помилка: проблема не в "одночасності"

Багато хто уявляє проблему так: два потоки _одночасно, в ту саму наносекунду_ намагаються записати щось в одну комірку пам'яті. Насправді це не зовсім так. Сучасні процесори виконують інструкції послідовно. Справжня проблема лежить не в апаратній площині, а в логічній, і її ім'я — **непередбачуване перемикання контексту**.

#### Справжня проблема: несподіване "засинання" потоку

Як ми пам'ятаємо, операційна система використовує витискаючу багатозадачність. Це означає, що вона може в **будь-який момент** призупинити виконання одного потоку і передати керування іншому. Потік може "заснути" і "прокинутися" в абсолютно непередбачуваний для програміста момент, навіть посередині однієї простої, на перший погляд, операції.

Саме ця непередбачуваність і породжує помилки, які називають **гонками станів (race conditions)**.

Розглянемо класичний приклад, що ілюструє цю проблему:

Уявімо, що два потоки (Потік А і Потік Б) працюють зі спільним вказівником на якийсь об'єкт у пам'яті.

1. **Крок 1 (Потік А):** Потік А перевіряє, чи вказівник `my_object` не є порожнім (`null`). Перевірка успішна, вказівник валідний. Потік А готується використати цей об'єкт.  
2. **Крок 2 (Перемикання):** _Саме в цей момент_ операційна система вирішує призупинити Потік А і передати керування Потоку Б.  
3. **Крок 3 (Потік Б):** Потік Б виконує свою роботу, яка полягає у звільненні ресурсів. Він видаляє об'єкт і встановлює вказівник `my_object` у `null`.  
4. **Крок 4 (Повернення):** Операційна система повертає керування Потоку А, який "прокидається" рівно в тому місці, де його перервали.  
5. **Крок 5 (Крах):** Потік А, який **вже** пройшов перевірку на `null`, впевнено намагається звернутися до об'єкта за вказівником `my_object`. Але вказівник вже порожній! Результат — помилка доступу до пам'яті (Access Violation) і крах програми.

**Висновок:** Проблема виникла не через те, що потоки одночасно щось робили, а через те, що стан системи змінився в проміжку часу між перевіркою умови та її використанням.

Ця фундаментальна проблема змусила розробників мов програмування шукати способи захистити внутрішні структури своїх інтерпретаторів та компіляторів. І в світі Python таким захисним механізмом став GIL.

### 4. Що таке GIL і чому він існує (захист інтерпретатора)

Фундаментальна проблема непередбачуваного перемикання потоків змусила розробників мов програмування шукати рішення. У світі CPython (стандартної реалізації Python) таким рішенням став **GIL — Global Interpreter Lock (Глобальне Блокування Інтерпретатора)**.

#### Що таке GIL?

По суті, GIL — це **м'ютекс** (механізм взаємного виключення), який захищає не ваш код, а **внутрішні структури самого інтерпретатора Python**. Його головне правило звучить так:

> **В будь-який момент часу лише один потік може виконувати байткод Python.**

Навіть якщо у вас 16-ядерний процесор і ви запустили 16 потоків, які виконують інтенсивні обчислення, GIL дозволить працювати лише одному з них. Решта 15 будуть чекати своєї черги.

#### Чому він існує? Головна причина — управління пам'яттю

Основна причина введення GIL — спрощення управління пам'яттю в CPython, зокрема, механізму **підрахунку посилань (reference counting)**. Кожен об'єкт у Python має лічильник, який показує, скільки змінних на нього посилаються. Коли лічильник доходить до нуля, об'єкт видаляється з пам'яті.

Уявімо, що було б без GIL:

1. Два потоки (А і Б) одночасно використовують один і той самий об'єкт. Лічильник посилань на нього дорівнює 2.  
2. Потік А завершує роботу з об'єктом і зменшує лічильник до 1.  
3. _В цей самий момент_ Потік Б також завершує роботу і теж зменшує лічильник до 1.  
4. Результат: обидва потоки зменшили лічильник з 2 до 1, хоча мали б зменшити його до 0. Об'єкт ніколи не буде видалений — **витік пам'яті**.

Щоб уникнути таких гонок станів, розробникам довелося б додавати блокування (локи) для кожної операції зі спільними даними по всьому коду інтерпретатора. Це б значно ускладнило розробку і потенційно сповільнило б роботу однопотокових програм.

GIL став простішим рішенням: одне глобальне блокування, яке гарантує, що операції над внутрішніми структурами даних, як-от підрахунок посилань, завжди є безпечними.

#### Як працює GIL?

Інтерпретатор Python працює за принципом "тиків" (ticks) — умовних одиниць виконаної роботи. Раніше потік виконував 100 тиків, після чого був змушений звільнити GIL. У сучасних версіях Python (3.2+) механізм змінився: перемикання відбувається приблизно кожні 5 мілісекунд, що робить реакцію програми більш чутливою.

Важливо розуміти, що **GIL звільняється під час блокуючих операцій вводу-виводу (I/O)**. Коли ваш потік починає читати файл з диска або чекати на відповідь від мережі, він віддає GIL іншим потокам. Саме тому багатопоточність у Python все ще є надзвичайно ефективною для I/O-задач.

**Ключовий висновок:**
- **GIL захищає інтерпретатор, а не ваш код.** Він гарантує, що сам Python не зламається, але він не рятує вас від логічних помилок (гонок станів) у вашій власній програмі. Для захисту ваших даних вам все одно потрібно використовувати локи та інші примітиви синхронізації.  
- **GIL є проблемою лише для CPU-bound задач**, які інтенсивно навантажують процесор. Для IO-bound задач він практично не створює перешкод.

### **IO-bound та CPU-bound задачі**

У багатопотоковості дуже важливо розуміти, для яких задач вона буде корисною, а для яких – навпаки, створюватиме проблеми. Всі задачі можна умовно розділити на дві категорії: ті, що залежать від швидкості обміну даними із зовнішніми системами (IO-bound), та ті, що потребують великих обчислювальних ресурсів (CPU-bound).

Задачі, які є IO-bound, це такі, де основний час витрачається на очікування відповіді від зовнішнього джерела. Це може бути запит до бази даних, читання або запис файлу на диск, скачування веб-сторінки або відправлення HTTP-запиту. У таких випадках процесор виконує мінімальну кількість роботи, а більшу частину часу простоює, очікуючи відповідь. Тут багатопоточність дійсно корисна, тому що навіть за наявності GIL один потік може передати управління іншому в той момент, коли сам простоює. Таким чином, програма може одночасно відправляти кілька запитів і отримувати відповіді набагато швидше, ніж якби всі запити виконувалися послідовно в одному потоці.

Для прикладу можна розглянути задачу, в якій потрібно завантажити десять веб-сторінок. Якщо робити це послідовно, кожен запит буде чекати відповіді перед тим, як відправити наступний. Натомість, використовуючи багатопоточність, можна запустити всі запити одночасно, і програма зможе працювати значно швидше. Ось код, який демонструє таку поведінку:

```python
import threading
import requests

urls = ["https://example.com" for _ in range(10)]

def fetch(url):
    resp = requests.get(url)
    print(f"Завантажено: {url}")

threads = [threading.Thread(target=fetch, args=(url,)) for url in urls]
for t in threads: t.start()
for t in threads: t.join()
```

Тут кожен потік буде займатися завантаженням окремого сайту, і загальний час виконання скоротиться, адже потоки працюватимуть одночасно, поки чекають відповіді від сервера.

На противагу цьому, CPU-bound задачі мають зовсім іншу природу. Це такі задачі, де головне навантаження припадає саме на процесор. Це можуть бути складні математичні обчислення, кодування відео, обробка великих масивів даних або моделювання фізичних процесів. У такому випадку головним обмеженням є можливості процесора, а не швидкість обміну даними з іншими системами.

Коли виконуються CPU-bound задачі, потоки не просто очікують на зовнішні ресурси, а активно використовують процесор. Тут GIL стає реальною проблемою, адже навіть якщо створити кілька потоків, вони не зможуть одночасно виконуватись на різних ядрах. Через це програма не отримує ніякого прискорення, а іноді навіть працює повільніше через витрати на перемикання контекстів між потоками. Саме тому в таких випадках багатопоточність у Python неефективна, і краще використовувати інший підхід – багатопроцесність.

Замість потоків можна створювати окремі процеси, кожен з яких буде незалежним і зможе використовувати повне навантаження на своєму ядрі. В Python для цього є бібліотека `multiprocessing`, яка дозволяє запускати кілька процесів і розподіляти між ними роботу. Наприклад, ось код, що демонструє використання пулу процесів для паралельних обчислень квадратів чисел:

```python
import multiprocessing

def square(n):
    return n * n

pool = multiprocessing.Pool(processes=4)
results = pool.map(square, range(10))
print(results)
```

Кожен процес у цьому випадку буде незалежним і працюватиме на окремому ядрі, що дозволить досягти справжнього паралелізму. Це показує, що для CPU-bound задач багатопроцесність значно ефективніша за багатопоточність, адже вона дозволяє повністю використовувати можливості багатоядерного процесора.

Таким чином, вибір між потоками та процесами залежить від типу задачі. Якщо програма активно взаємодіє з мережею, базою даних або файловою системою, багатопоточність буде хорошим рішенням. Якщо ж головне навантаження припадає на процесор, варто використовувати multiprocessing. Розуміння цієї різниці допомагає писати ефективнішу та продуктивнішу програму.

### **Синхронізація потоків**

Коли кілька потоків працюють одночасно та звертаються до спільних ресурсів, виникає проблема синхронізації. Якщо кілька потоків одночасно змінюють спільні дані, може відбутися ситуація, коли результат буде непередбачуваним. Це відоме як "гонка станів" (race condition), і воно може призвести до некоректної роботи програми або навіть до серйозних помилок.

Уявімо, що у нас є глобальна змінна-лічильник, яка збільшується різними потоками. Якщо не контролювати доступ до цієї змінної, можна отримати неправильні результати. Наприклад, два потоки можуть одночасно зчитати значення, обчислити його нове значення і записати його назад, не враховуючи зміни, які зробив інший потік. Це призведе до втрати деяких інкрементів.

Щоб уникнути подібних проблем, використовують механізми синхронізації. В Python є кілька основних способів синхронізації потоків: локи (Locks), м’ютекси (Mutexes) та семафори (Semaphores).

**Локи** – це найпростіший спосіб забезпечити взаємовиключний доступ до ресурсу. Коли один потік отримує лок, інші потоки не можуть отримати доступ до ресурсу, поки лок не буде звільнений. Це дозволяє гарантувати, що тільки один потік працює з ресурсом у певний момент часу. Приклад використання локів у Python виглядає так:

```python
import threading

lock = threading.Lock()
shared_resource = 0

def worker():
    global shared_resource
    with lock:
        temp = shared_resource
        temp += 1
        shared_resource = temp

threads = [threading.Thread(target=worker) for _ in range(10)]
for t in threads: t.start()
for t in threads: t.join()

print("Фінальне значення:", shared_resource)
```

Тут використовується `with lock:`, що є більш безпечним способом роботи з локами, оскільки гарантує автоматичне звільнення ресурсу після виходу з блоку коду.

**М’ютекси** працюють схоже на локи, але мають додатковий захист від повторного блокування самим потоком. Якщо потік уже заблокував м’ютекс, він не може заблокувати його повторно, поки не звільнить його. Це запобігає потенційним проблемам, коли один і той самий потік намагається отримати ресурс двічі, що може призвести до взаємного блокування (deadlock).

**Семафори** використовуються, коли потрібно обмежити кількість потоків, які можуть одночасно працювати з ресурсом. Наприклад, якщо у нас є база даних, яка підтримує одночасно лише два з’єднання, семафор допоможе обмежити кількість потоків, які можуть одночасно звертатися до бази. Ось як це виглядає в коді:

```python
import threading
import time
import random

semaphore = threading.Semaphore(2)

def worker(name):
    with semaphore:
        print(f"{name} отримав ресурс")
        time.sleep(random.randint(1, 3))
        print(f"{name} звільнив ресурс")

threads = [threading.Thread(target=worker, args=(f"Потік-{i}",)) for i in range(5)]
for t in threads: t.start()
for t in threads: t.join()
```

У цьому прикладі тільки два потоки одночасно можуть отримати доступ до ресурсу. Інші потоки змушені чекати, поки один із попередніх потоків звільнить семафор.

Синхронізація потоків – це важлива частина багатопотокового програмування, адже неправильне управління доступом до ресурсів може призвести до непередбачуваних помилок. Вибір між локами, м’ютексами та семафорами залежить від конкретного сценарію: локи підходять для простого блокування доступу, м’ютекси корисні для складніших ситуацій, коли потік може повторно блокувати ресурс, а семафори дозволяють контролювати кількість потоків, які можуть одночасно працювати з ресурсом.

### **Локи, рекурсивні локи, м’ютекси та семафори**

#### **Локи (Locks)**

Лок (`threading.Lock()`) є базовим механізмом синхронізації потоків у Python. Його головна задача – забезпечити взаємовиключний доступ до ресурсу. Коли один потік отримує лок, інші потоки змушені чекати, поки цей лок не буде звільнений. Це гарантує, що одночасно тільки один потік може працювати з критичною секцією коду, тим самим запобігаючи гонкам станів та некоректним змінам даних.

Локи в Python можуть використовуватись у двох основних варіантах: явне блокування та автоматичне керування через `with`.

При явному блокуванні потік викликає `lock.acquire()`, після чого інші потоки змушені чекати, поки поточний потік не виконає `lock.release()`.

```python
import threading

lock = threading.Lock()
shared_resource = 0

def worker():
    global shared_resource
    lock.acquire()  # Захоплюємо лок
    shared_resource += 1
    lock.release()  # Звільняємо лок

threads = [threading.Thread(target=worker) for _ in range(10)]
for t in threads: t.start()
for t in threads: t.join()

print("Фінальне значення:", shared_resource)
```

Однак такий підхід може бути небезпечним: якщо в коді між `acquire()` і `release()` виникне помилка або виняток, лок може ніколи не бути звільнений, і програма зависне.

Більш безпечним є використання `with lock:`. У цьому випадку навіть якщо всередині блоку виникне помилка, лок автоматично звільниться.

```python
import threading

lock = threading.Lock()
shared_resource = 0

def worker():
    global shared_resource
    with lock:  # Безпечне блокування
        shared_resource += 1

threads = [threading.Thread(target=worker) for _ in range(10)]
for t in threads: t.start()
for t in threads: t.join()

print("Фінальне значення:", shared_resource)
```

#### **Рекурсивні локи (RLock)**

Рекурсивний лок (`threading.RLock()`) відрізняється від звичайного локa тим, що його може повторно заблокувати той самий потік без виникнення deadlock'у.

Звичайний лок не дозволяє одному й тому ж потоку заблокувати себе вдруге, оскільки це призвело б до зависання. Уявімо, що потік вже виконав `lock.acquire()`, а потім викликає ту ж функцію, яка знову намагається захопити цей лок. Це спричиняє безкінечне очікування, оскільки потік сам блокує себе.

Рекурсивний лок вирішує цю проблему, дозволяючи одному й тому ж потоку отримати блокування кілька разів, поки воно не буде повністю звільнене. Це корисно в ситуаціях, коли функція з локом викликає іншу функцію, яка також використовує той самий лок.

```python
import threading

rlock = threading.RLock()

def recursive_function(n):
    if n <= 0:
        return
    with rlock:
        print(f"Виклик {n}")
        recursive_function(n - 1)

recursive_function(5)
```

Тут без `RLock` програма зависла б, оскільки кожен рекурсивний виклик знову намагався б отримати лок.

#### **М’ютекси (Mutex – Mutual Exclusion)**

М’ютекс – це концептуально той самий механізм, що і лок, але використовується в ширшому контексті. У Python м’ютекси реалізуються через `threading.Lock()`. Вони гарантують, що тільки один потік може отримати доступ до ресурсу в певний момент часу.

Основна різниця між м’ютексом і звичайним локом полягає у використанні між процесами. Якщо лок використовується тільки в межах одного процесу, то м’ютекси можуть використовуватися для міжпроцесного блокування (через `multiprocessing.Lock()`).

Ось приклад використання м’ютекса для спільного доступу до ресурсів у багатопотоковому середовищі:

```python
import threading
import time

mutex = threading.Lock()

def critical_section(name):
    with mutex:
        print(f"{name} отримав доступ до ресурсу")
        time.sleep(2)
        print(f"{name} звільнив ресурс")

threads = [threading.Thread(target=critical_section, args=(f"Потік-{i}",)) for i in range(3)]
for t in threads: t.start()
for t in threads: t.join()
```

Тут кожен потік отримує доступ до спільного ресурсу, але інші потоки змушені чекати, поки попередній потік звільнить блокування.

#### **Семафори (Semaphores)**

Семафор (`threading.Semaphore()`) – це узагальнення м’ютекса, яке дозволяє кільком потокам одночасно отримати доступ до ресурсу. На відміну від звичайного локa, який дозволяє лише одному потоку мати доступ до критичної секції, семафор дозволяє обмежену кількість одночасних доступів.

Наприклад, якщо у вас є база даних, яка підтримує максимум три одночасні підключення, можна використовувати семафор із значенням 3. Це означає, що одночасно три потоки можуть отримати доступ, а інші будуть чекати, поки місце звільниться.

```python
import threading
import time
import random

semaphore = threading.Semaphore(2)

def worker(name):
    with semaphore:
        print(f"{name} отримав ресурс")
        time.sleep(random.randint(1, 3))
        print(f"{name} звільнив ресурс")

threads = [threading.Thread(target=worker, args=(f"Потік-{i}",)) for i in range(5)]
for t in threads: t.start()
for t in threads: t.join()
```

У цьому коді одночасно можуть працювати лише два потоки, навіть якщо ми створили п’ять потоків. Третій, четвертий і п’ятий потоки змушені чекати, поки хтось із перших двох звільнить семафор.

Окрім звичайного семафора, є також **лічильниковий семафор (`threading.BoundedSemaphore()`)**, який працює аналогічно, але з додатковим захистом. Якщо один із потоків випадково викличе `release()` більше разів, ніж було `acquire()`, звичайний семафор дозволить це зробити, а `BoundedSemaphore` викличе помилку. Це допомагає запобігати помилкам у логіці програми.

### **Чому GIL не робить механізми синхронізації зайвими?**

#### **1. GIL не гарантує послідовний доступ до змінних у багатопотоковому коді**

Хоча GIL запобігає одночасному виконанню двох потоків на рівні інтерпретатора Python, він не гарантує атомарність операцій у багатопотоковому середовищі. Наприклад, операції над змінними, які здаються "простими", насправді можуть виконуватися в кілька етапів.

Розглянемо приклад:

```python
import threading

counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1

threads = [threading.Thread(target=increment) for _ in range(2)]
for t in threads: t.start()
for t in threads: t.join()

print(counter)  # Очікуємо 200000, але результат може бути меншим
```

Операція `counter += 1` насправді виконується у три етапи:

1. Зчитування поточного значення `counter`
2. Додавання 1
3. Запис оновленого значення назад

Якщо між цими етапами GIL переключить контекст і інший потік виконає ту ж операцію, результати можуть бути некоректними. Через це навіть у Python необхідно використовувати **локи** для захисту критичних секцій.

```python
import threading

counter = 0
lock = threading.Lock()

def increment():
    global counter
    for _ in range(100000):
        with lock:
            counter += 1

threads = [threading.Thread(target=increment) for _ in range(2)]
for t in threads: t.start()
for t in threads: t.join()

print(counter)  # Завжди 200000
```

Тут `lock` гарантує, що операція `counter += 1` виконується атомарно, не допускаючи зміни змінної одночасно двома потоками.

---

#### **2. GIL працює лише в CPython, але не у всіх реалізаціях Python**

GIL є особливістю CPython (стандартної імплементації Python), але інші версії Python, такі як Jython (на Java) та IronPython (на .NET), не мають GIL. Якщо писати код із припущенням, що GIL "захистить" від проблем паралелізму, цей код може виявитися некоректним у середовищах без GIL.

Якщо в майбутньому Python без GIL (PEP 703) стане стандартом, програми, які раніше покладалися на GIL, можуть почати працювати некоректно. Використання явних механізмів синхронізації зробить код стійкішим до таких змін.

---

#### **3. GIL не впливає на зовнішні ресурси (взаємодія з файлами, мережею, базами даних)**

Навіть у середовищі з GIL багатопотокові програми можуть одночасно працювати з ресурсами, які не контролюються GIL. Наприклад, якщо два потоки пишуть у файл або взаємодіють з базою даних, GIL не регулює їхню роботу.

Уявімо ситуацію, коли два потоки записують лог-файли:

```python
import threading

def write_log():
    # Явно вказуємо кодування UTF-8
    with open("log.txt", "a", encoding="utf-8") as f:
        for _ in range(100):
            f.write("Логування з потоку\n")

threads = [threading.Thread(target=write_log) for _ in range(5)]
for t in threads: t.start()
for t in threads: t.join()

print("Роботу завершено. Перевірте файл log.txt")
```

Цей код може спричинити хаос у файлі, оскільки потоки можуть записувати рядки одночасно, що призведе до пошкоджених або злитих записів.

Використовуючи лок, можна гарантувати, що кожен потік записуватиме у файл без перетинання з іншими:

```python
import threading

lock = threading.Lock()

def write_log():
    with lock:
        with open("log.txt", "a") as f:
            for _ in range(100):
                f.write("Логування з потоку\n")

threads = [threading.Thread(target=write_log) for _ in range(5)]
for t in threads: t.start()
for t in threads: t.join()
```

Завдяки `lock`, тільки один потік одночасно виконує запис у файл, що запобігає конфліктам.

---
### **Механізм роботи GIL у CPython**

GIL (Global Interpreter Lock) — це глобальне блокування інтерпретатора Python, яке регулює виконання потоків у CPython. Його головна функція — гарантувати, що в будь-який момент часу лише один потік виконує байткод Python, навіть якщо в системі є кілька ядер процесора.

Щоб зрозуміти, як саме працює GIL, розглянемо його основні компоненти та механізми, які визначають порядок виконання потоків у CPython.

---

### **1. Основні компоненти GIL**

GIL працює за допомогою таких основних елементів:

- **Глобальна змінна `gil`**  
    У CPython GIL представлений як глобальна змінна у вихідному коді інтерпретатора. Це змінна, яка вказує, чи дозволено поточному потоку виконувати байткод Python.
- **Механізм блокування (Lock)**  
    GIL реалізований як **м’ютекс (mutex)** — взаємовиключний механізм, який дозволяє одному потоку виконувати код, а інші змушує чекати.
- **Таймер перемикання потоків**  
    Python використовує механізм **контекстного перемикання** між потоками. Через певний проміжок часу один потік звільняє GIL, щоб інші потоки могли отримати можливість виконання.
- **Сигнали `Py_BEGIN_ALLOW_THREADS` і `Py_END_ALLOW_THREADS`**  
    У коді CPython є спеціальні макроси, які дозволяють тимчасово звільняти GIL для виконання операцій, що не залежать від Python (наприклад, очікування введення-виведення).

---

### **2. Процес роботи GIL у багатопотоковій програмі**

GIL працює циклічно:

1. **Один потік отримує GIL**  
    Коли потік запускається, він повинен отримати GIL. Якщо GIL зайнятий іншим потоком, потік змушений чекати, поки той його звільнить.
2. **Потік виконує певну кількість інструкцій**  
    Потік виконує байткод Python, поки не виконає певну кількість інструкцій (це називається **GIL quantum**, за замовчуванням близько 5 мілісекунд).
3. **GIL тимчасово звільняється**  
    Після закінчення квантового часу потік тимчасово звільняє GIL, щоб інші потоки могли отримати можливість виконання. Якщо жоден інший потік не чекає, той самий потік може знову отримати GIL.
4. **Якщо потік очікує I/O, він звільняє GIL**  
    Якщо потік виконує операцію введення-виведення (наприклад, читає файл або очікує відповідь від сервера), він викликає `Py_BEGIN_ALLOW_THREADS`, щоб тимчасово звільнити GIL. Інші потоки можуть у цей час виконуватися.
5. **Інший потік отримує GIL**  
		    Якщо є інші потоки, вони можуть отримати GIL після перемикання контексту. Якщо ні, потік, який виконувався раніше, просто продовжить роботу.

Процес повторюється циклічно, що створює ефект псевдопаралельного виконання потоків.

---

### **3. Як виглядає GIL у вихідному коді CPython?**

Якщо заглянути у вихідний код CPython (файл `ceval.c`), можна побачити механізм GIL. Основний механізм контролю потоків виглядає так:

```c
/* Отримання GIL */
PyEval_AcquireLock();

/* Виконання байткоду */
while (opcodes_left > 0) {
    execute_next_opcode();
    opcodes_left--;
}

/* Звільнення GIL */
PyEval_ReleaseLock();
```

Коли потік отримує GIL (`PyEval_AcquireLock()`), він починає виконувати інструкції Python. Після досягнення певної кількості інструкцій або після запиту на введення-виведення (`PyEval_ReleaseLock()`), GIL звільняється і може бути отриманий іншим потоком.

У CPython є механізм **thread switching**, який виконує перевірку, чи потрібно перемикати потік, кожні 5 мілісекунд:

```c
if (--gil_interval <= 0) {
    gil_interval = DEFAULT_INTERVAL;
    PyThreadState_Swap(next_thread);
}
```

Тут `gil_interval` визначає, як довго потік може утримувати GIL, перш ніж відбудеться примусове перемикання контексту.

---

### **4. Взаємодія GIL з введенням-виведенням (I/O-bound задачі)**

GIL блокує багатопотоковість для обчислювальних задач, але не для операцій введення-виведення. Коли потік виконує операцію, яка не потребує обчислень (наприклад, очікує відповіді від мережі), Python дозволяє іншим потокам отримати GIL.

Це реалізується за допомогою наступних макросів у вихідному коді CPython:

```c
Py_BEGIN_ALLOW_THREADS;
/* Операція, яка не використовує Python */
do_something_that_takes_time();
Py_END_ALLOW_THREADS;
```

Коли Python зустрічає `Py_BEGIN_ALLOW_THREADS`, він тимчасово звільняє GIL, дозволяючи іншим потокам виконуватись. Коли I/O-операція завершується, потік повторно отримує GIL (`Py_END_ALLOW_THREADS`) і продовжує виконання.

### **Практичні приклади багатопотоковості в Python**

Тепер, коли механізм роботи GIL зрозумілий, розглянемо кілька практичних прикладів, які демонструють, як працює багатопотоковість у Python. Ми розглянемо:

1. **Просту програму з потоками**
2. **Вплив GIL на CPU-bound задачі**
3. **Правильне використання локів**
4. **Приклад роботи з семафорами**
5. **Оптимальне рішення для CPU-bound задач за допомогою multiprocessing**

---

### **1. Запуск потоків у Python**

Розглянемо найпростіший приклад багатопотоковості, де створюється кілька потоків, які виконують однакову функцію.

```python
import threading
import time

def worker(n):
    print(f"Потік {n} почав роботу")
    time.sleep(2)
    print(f"Потік {n} завершив роботу")

threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]

for t in threads:
    t.start()

for t in threads:
    t.join()

print("Всі потоки завершили роботу")
```

Тут створюються п’ять потоків, кожен з яких виконується незалежно і завершує свою роботу через 2 секунди. Однак через GIL ці потоки не виконуються одночасно в справжньому паралельному режимі, а лише чергуються.

---

### **2. Вплив GIL на CPU-bound задачі**

Давайте подивимося, як GIL обмежує продуктивність у випадку інтенсивних обчислень. Наприклад, обчислимо суму чисел від 1 до 50_000_000 у кількох потоках.

```python
import threading
import time

COUNT = 50_000_000

def countdown(n):
    while n > 0:
        n -= 1

t1 = threading.Thread(target=countdown, args=(COUNT//2,))
t2 = threading.Thread(target=countdown, args=(COUNT//2,))

start = time.time()
t1.start()
t2.start()
t1.join()
t2.join()
end = time.time()

print(f"Час виконання: {end - start:.2f} секунд")
```

Очікувано, що два потоки мали б поділити роботу і виконати її вдвічі швидше, однак через GIL вони не можуть виконуватися паралельно, тому час виконання майже не зміниться порівняно з виконанням у одному потоці.

---

### **3. Використання локів для запобігання race condition**

Уявімо ситуацію, коли кілька потоків одночасно модифікують глобальну змінну, наприклад, лічильник.

```python
import threading

counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1  # Операція не атомарна!

threads = [threading.Thread(target=increment) for _ in range(5)]

for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"Очікуваний результат: {5 * 100000}, реальний: {counter}")
```

Те, що код спрацював правильно локально, не означає, що він правильний. Вам просто "пощастило" з таймінгами та умовами.

### Детальне пояснення

Цей приклад покликаний показати **гонку станів (race condition)**. Проблема в тому, що гонки станів є **недетермінованими**. Їх прояв залежить від того, як планувальник операційної системи та GIL в Python вирішать перемикати контекст між потоками.

Давайте розберемо, чому у вашому випадку результат міг виявитися правильним:

1. **Швидкість операції:** Операція `counter += 1` хоч і не атомарна, але виконується надзвичайно швидко. Вона складається з трьох байткод-інструкцій: завантажити значення, збільшити його, зберегти назад. Імовірність того, що GIL перемкне потік саме між цими трьома інструкціями, існує, але вона не дорівнює 100%.
2. **Робота GIL:** GIL перемикає потоки приблизно кожні 5 мілісекунд (у сучасних версіях Python). За цей час ваш процесор може встигнути виконати цикл `for` тисячі разів без перемикання. Могло статися так, що перемикання контексту відбувалося переважно _між_ ітераціями циклу, а не _всередині_ операції `counter += 1`.
3. **Поточне навантаження системи:** На результат впливає загальне навантаження на ваш процесор. На менш завантаженій системі перемикань може бути менше.

```python
import threading
import time

counter = 0

def increment():
    global counter
    for _ in range(100000):
        # Зберігаємо поточне значення у локальну змінну
        local_counter = counter
        
        # Штучно створюємо "простір" для перемикання контексту
        time.sleep(0) 
        
        # Збільшуємо локальне значення
        local_counter += 1
        
        # Зберігаємо його назад
        counter = local_counter

threads = [threading.Thread(target=increment) for _ in range(5)]
for t in threads: t.start()
for t in threads: t.join()

print(f"Очікуваний результат: 500000, реальний: {counter}")
```
Через те, що `counter += 1` — це не атомарна операція (складається із зчитування, збільшення та запису значення), можливі втрати інкрементів через race condition.

Правильний варіант із використанням `Lock`:
```python
import threading
import time

# Створюємо екземпляр блокування
lock = threading.Lock()
counter = 0

def increment():
    global counter
    for _ in range(100000):
        # Захоплюємо блокування перед входом у критичну секцію
        with lock:
            # --- ПОЧАТОК КРИТИЧНОЇ СЕКЦІЇ ---
            # Тепер весь цей блок коду є атомарним.
            
            local_counter = counter
            time.sleep(0) 
            local_counter += 1
            counter = local_counter
            # --- КІНЕЦЬ КРИТИЧНОЇ СЕКЦІЇ ---
        # Блокування автоматично звільняється при виході з блоку 'with'

threads = [threading.Thread(target=increment) for _ in range(5)]
for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"Очікуваний результат: 500000, реальний: {counter}")
```

Тепер результат буде коректним, оскільки тільки один потік може змінювати `counter` у певний момент часу.

---

### **4. Використання семафорів для обмеження доступу**

Семафори дозволяють обмежити кількість потоків, які одночасно отримують доступ до ресурсу. Наприклад, якщо ми хочемо, щоб одночасно працювали не більше двох потоків:

```python
import threading
import time

semaphore = threading.Semaphore(2)

def worker(n):
    with semaphore:
        print(f"Потік {n} отримав доступ до ресурсу")
        time.sleep(2)
        print(f"Потік {n} звільнив ресурс")

threads = [threading.Thread(target=worker, args=(i,)) for i in range(5)]

for t in threads:
    t.start()
for t in threads:
    t.join()
```

Тут одночасно виконуватимуться тільки два потоки, решта будуть чекати, поки один із перших двох не звільнить семафор.

---

### **5. Використання multiprocessing для CPU-bound задач**

Оскільки GIL не дозволяє багатопотоковість для обчислювальних задач, можна замість `threading` використовувати `multiprocessing`, де кожен процес буде виконуватися окремо і використовувати своє ядро процесора.

Той самий приклад підрахунку чисел, але з `multiprocessing`:

```python
import multiprocessing
import time

COUNT = 50_000_000

def countdown(n):
    while n > 0:
        n -= 1

p1 = multiprocessing.Process(target=countdown, args=(COUNT//2,))
p2 = multiprocessing.Process(target=countdown, args=(COUNT//2,))

start = time.time()
p1.start()
p2.start()
p1.join()
p2.join()
end = time.time()

print(f"Час виконання: {end - start:.2f} секунд")
```

На багатоядерному процесорі цей код працюватиме майже вдвічі швидше, оскільки `multiprocessing` обходить GIL, створюючи окремі процеси.

---

