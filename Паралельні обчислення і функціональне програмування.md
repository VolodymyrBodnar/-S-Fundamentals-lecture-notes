
### **1. Вступ**

У сучасному програмуванні зростає потреба в ефективному опрацюванні великих обсягів даних. Послідовне виконання операцій, коли програма обробляє кожен елемент по черзі, стає серйозним обмеженням для масштабованих систем. Один із підходів до вирішення цієї проблеми — **паралельні обчислення**, що дозволяють розділити завдання на кілька частин і обробляти їх одночасно. Це особливо важливо для задач, що виконуються на багатоядерних процесорах або розподілених кластерах.

Розробка програм, що підтримують паралельне виконання, традиційно пов’язана з труднощами: необхідно враховувати взаємодію між потоками, контроль стану змінних і обробку конфліктів доступу до пам’яті. Одним із рішень цих проблем є **функціональне програмування**, оскільки воно мінімізує побічні ефекти та змінюваний стан, що робить його ідеальним для розпаралелювання.

Функціональне програмування базується на обробці даних через **чисті функції**, які не змінюють зовнішній стан і завжди повертають однаковий результат для однакових вхідних даних. Завдяки цьому такі функції можна легко виконувати паралельно, оскільки вони не залежать одна від одної і не створюють конфліктів при доступі до ресурсів.

Особливо важливими для паралельних обчислень є три ключові концепції функціонального програмування: **map**, **reduce** та **filter**. Вони дозволяють розподіляти обчислення між незалежними процесами або вузлами, використовуючи декларативний підхід. Функція **map** застосовує обчислення до кожного елемента колекції незалежно, **reduce** агрегує результати, а **filter** дозволяє відбирати необхідні дані. Завдяки цим операціям можна ефективно організувати паралельну обробку, уникнувши складнощів класичного потокового програмування.

Ці принципи стали основою для технології **MapReduce**, яка широко використовується у великих розподілених системах для аналізу та обробки великих даних. Вона дозволяє розділити великі завдання на підзадачі, розподілити їх між вузлами кластера та об’єднати результати в єдиний підсумок.

У цій лекції ми спочатку розглянемо функціональне програмування та його ключові концепції у контексті паралельних обчислень, потім перейдемо до глибшого аналізу технології MapReduce і розберемо її реалізацію на практичному прикладі.

  

## **Функціональне програмування для аналізу даних: чому це важливо?**


Для **Data Engineer** та **Data Scientist** робота з даними є щоденною рутиною. Вони будують складні конвеєри обробки (data pipelines), трансформують величезні масиви інформації та створюють моделі, що вимагають відтворюваних результатів. У цьому контексті імперативний підхід, де стан програми постійно змінюється, може призвести до помилок, які важко відстежити, та ускладнити паралельну обробку.

Саме тут **функціональне програмування (ФП)** стає надзвичайно доцільним. Його ключові принципи ніби створені для розв'язання типових проблем у роботі з даними.

- **Надійність та передбачуваність.** Коли ваші функції не змінюють зовнішні дані та не залежать від них (чисті функції), ви завжди отримуєте очікуваний результат. Ваш конвеєр обробки даних стає стабільним, як швейцарський годинник.
- **Простота тестування та зневадження.** Тестувати чисту функцію — одне задоволення. Ви просто подаєте їй вхідні дані й перевіряєте результат. Ніяких складних налаштувань "стану системи" не потрібно. Якщо в конвеєрі виникла помилка, знайти її джерело набагато простіше.
- **Легкість паралелізації.** Оскільки функції не мають побічних ефектів і не конкурують за спільні ресурси, їх можна виконувати паралельно без ризику виникнення конфліктів. Це фундаментальна перевага для обробки великих даних на багатоядерних процесорах або у розподілених системах.

---

### **Основні поняття функціонального програмування в Python**

#### **1. Чисті функції (Pure Functions)**

```python

# Приклад чистої функції

def calculate_price_with_tax(price, tax_rate):
    return price * (1 + tax_rate)

  

# Приклад НЕ чистої функції (має побічний ефект)
user_discounts = {"user1": 0.1, "user2": 0.2}

  

def apply_discount(user_id, price):
    discount = user_discounts.get(user_id, 0)
    final_price = price * (1 - discount)
    user_discounts[user_id] = 0  # побічний ефект
    return final_price

```

  

#### **2. Незмінність даних (Immutability)**

```python
# Поганий підхід (змінність)
def add_tag_mutably(items, tag):
    for i in range(len(items)):
        items[i] += f"_{tag}"
    return items
  

# Гарний підхід (незмінність)

def add_tag_immutably(items, tag):
    return [item + f"_{tag}" for item in items]


products = ["apple", "banana"]
new_products = add_tag_immutably(products, "fruit")
```
#### **3. Функції вищого порядку (Higher-Order Functions)**

```python

def process_data(data, processor_func):
    print(f"Обробляємо дані за допомогою {processor_func.__name__}...")
    return [processor_func(item) for item in data]
 
def to_uppercase(text):
    return text.upper()

def add_timestamp(text):
    from datetime import datetime
    return f"{text} (at {datetime.now().time()})"

my_data = ["event1", "event2"]
print(process_data(my_data, to_uppercase))
```
#### **4. Композиція функцій (Function Composition)**
```python

def normalize_text(text):
    return text.strip().lower()
  

def wrap_in_tags(text, tag="p"):
    return f"<{tag}>{text}</{tag}>"
  

raw_text = "  Some important Data  "
normalized = normalize_text(raw_text)
html_output = wrap_in_tags(normalized)

def compose(f, g):
    return lambda x: f(g(x))

format_as_html = compose(wrap_in_tags, normalize_text)
print(format_as_html("  Another Example  "))
```
  
Для аналітика даних композиція дозволяє будувати складні конвеєри трансформацій з простих, атомарних кроків.
### **2. Функції першого класу у функціональному програмуванні**

Функціональне програмування базується на кількох фундаментальних концепціях, і однією з найважливіших є **функції першого класу**. Це означає, що у програмній мові функції можна використовувати так само, як і будь-які інші змінні. Вони можуть передаватися як аргументи в інші функції, зберігатися у структурах даних або навіть повертатися як результат виконання іншої функції.

Розглянемо цей принцип на прикладі Python:

```python
def square(x):
    return x * x

print(square(5))  # Виведе: 25

# Присвоєння функції змінній
f = square
print(f(5))  # Виведе: 25

# Передача функції як аргумент
def apply_function(func, value):
    return func(value)

print(apply_function(square, 6))  # Виведе: 36
```

Тут ми бачимо, що функція `square` може бути присвоєна змінній `f` і викликана через неї. Крім того, ми можемо передати її як аргумент у функцію `apply_function`, яка отримує іншу функцію і застосовує її до значення.

Цей підхід відкриває можливість створювати більш гнучкі та універсальні функції. Наприклад, замість того, щоб писати окремі реалізації для різних операцій, ми можемо передавати функцію, яка визначає поведінку:

```python
def add_one(x):
    return x + 1

def multiply_by_two(x):
    return x * 2

operations = [add_one, multiply_by_two, square]

for op in operations:
    print(op(3))  # Виведе: 4, 6, 9
```

Ще одна важлива властивість функцій першого класу — можливість повертати функції як результат. Це особливо корисно для створення динамічних обчислювальних процесів:

```python
def make_multiplier(n):
    def multiplier(x):
        return x * n
    return multiplier

times_three = make_multiplier(3)
print(times_three(10))  # Виведе: 30
```

Це дозволяє створювати узагальнені функції, які можна легко адаптувати до конкретних завдань. Наприклад, замість того, щоб жорстко задавати множник, ми можемо створювати нові функції на льоту, передаючи потрібні параметри.

Функції першого класу є основою для **вищих функцій порядку**, таких як `map`, `reduce` і `filter`. Вони дозволяють маніпулювати колекціями даних декларативним способом, що робить код більш зрозумілим та ефективним, особливо в контексті паралельних обчислень. Наступним кроком ми розглянемо, як ці функції працюють і як вони використовуються у практичних задачах.

### **3. Основні функції функціонального програмування (20 хвилин)**

Функціональне програмування пропонує набір інструментів, які спрощують роботу з колекціями даних та дозволяють легко паралелізувати обчислення. Серед них особливе місце займають три ключові операції: **map**, **reduce** та **filter**. Ці функції широко застосовуються у паралельних обчисленнях, оскільки вони дозволяють виконувати обчислення без зміни стану програми та без взаємодії між потоками, що є критично важливим для ефективної роботи на багатоядерних системах і розподілених кластерах.

#### **Функція map**

Функція `map` виконує однакову операцію над кожним елементом колекції, не змінюючи її структури. Це дозволяє виконувати обчислення над кожним елементом незалежно, що ідеально підходить для паралельної обробки. В контексті розподілених обчислень це означає, що можна розділити набір даних на частини і обробляти їх на різних процесорах або серверах одночасно.

Розглянемо простий приклад: піднесення кожного елемента списку до квадрату.

```python
numbers = [1, 2, 3, 4, 5]

# Використання функції map
squared_numbers = list(map(lambda x: x ** 2, numbers))

print(squared_numbers)  # Виведе: [1, 4, 9, 16, 25]
```

Кожен елемент списку обробляється окремо, а результатом є новий список з обчисленими значеннями. Цей процес легко розпаралелюється, оскільки жодна з операцій не залежить від іншої.

Якщо потрібно виконати ці обчислення паралельно, можна використати бібліотеку `multiprocessing`, яка дозволяє обробляти дані в декількох процесах:

```python
from multiprocessing import Pool

def square(x):
    return x ** 2

with Pool() as pool:
    squared_numbers = pool.map(square, numbers)

print(squared_numbers)  # Виведе: [1, 4, 9, 16, 25]
```

Тут ми створюємо пул процесів і застосовуємо функцію `square` до кожного елемента списку одночасно, що значно пришвидшує обчислення на багатоядерних процесорах.

#### **Функція reduce**

Функція `reduce` використовується для агрегування результатів у єдине значення. Вона ідеально підходить для завдань, де потрібно обчислити суму, добуток, максимум або інші агреговані значення для великого набору даних.

Ось приклад використання `reduce` для підрахунку суми всіх елементів у списку:

```python
from functools import reduce

numbers = [1, 2, 3, 4, 5]

sum_of_numbers = reduce(lambda x, y: x + y, numbers)

print(sum_of_numbers)  # Виведе: 15
```

Ця операція поступово комбінує елементи списку: спочатку складає перші два, потім результат додає до наступного елемента і так далі.

У контексті паралельних обчислень `reduce` може бути розбитий на кілька етапів: спочатку обчислення виконується на різних підмножинах даних, а потім часткові результати об’єднуються в єдиний підсумок. Це є ключовою ідеєю функції **Reduce** у MapReduce.

#### **Функція filter**

Функція `filter` використовується для вибору елементів, які задовольняють певну умову. Це дуже корисно при роботі з великими наборами даних, коли потрібно відфільтрувати лише необхідні елементи перед їх подальшою обробкою.

Розглянемо приклад фільтрації парних чисел:

```python
numbers = [1, 2, 3, 4, 5, 6, 7, 8]

even_numbers = list(filter(lambda x: x % 2 == 0, numbers))

print(even_numbers)  # Виведе: [2, 4, 6, 8]
```

Як і `map`, функція `filter` може бути розпаралелена, якщо обробка кожного елемента не залежить від інших.

#### **Як ці функції працюють у паралельних обчисленнях?**

Функції `map`, `reduce` і `filter` дозволяють описувати обчислення декларативно, тобто визначати, що потрібно зробити, а не як саме це реалізовано. Завдяки цьому вони ідеально підходять для розподілених систем. Наприклад, у системах обробки великих даних, таких як Hadoop або Spark, функція `map` може застосовуватися незалежно до різних частин даних на різних вузлах, `filter` може прибирати зайві дані ще на початкових етапах обробки, а `reduce` об’єднувати часткові результати після виконання обчислень.

У наступному розділі ми розглянемо, як ці принципи використовуються в технології MapReduce, яка дозволяє масштабувати обчислення на тисячі серверів та ефективно працювати з великими обсягами даних.

У бібліотеці `multiprocessing` підтримка `map` реалізована через `Pool.map()`, що дозволяє легко розпаралелити виконання функції для кожного елемента списку. Проте `multiprocessing` **не має вбудованої підтримки `filter` та `reduce`**. Однак ці операції можна реалізувати вручну, використовуючи `Pool.map()` у поєднанні з додатковою логікою.

### **Паралельний filter з multiprocessing**

Оскільки `filter` застосовується до кожного елемента колекції незалежно, його можна реалізувати через `map`, а потім видалити `None` або `False` значення.

```python
from multiprocessing import Pool

numbers = [1, 2, 3, 4, 5, 6, 7, 8]

def is_even(x):
    return x if x % 2 == 0 else None  # Повертаємо None для непарних чисел

with Pool() as pool:
    filtered_results = pool.map(is_even, numbers)

even_numbers = [x for x in filtered_results if x is not None]

print(even_numbers)  # Виведе: [2, 4, 6, 8]
```

Тут ми використовуємо `map`, щоб перевірити кожен елемент списку, а потім вручну фільтруємо отримані значення.

---

### **Паралельний reduce з multiprocessing**

Оскільки `reduce` передбачає поступове агрегування значень, його можна реалізувати через `Pool.map()`, розбивши дані на блоки, а потім об’єднавши часткові результати.

```python
from multiprocessing import Pool
from functools import reduce
  
numbers = [1, 2, 3, 4, 5, 6, 7, 8]


def chunk_sum(sublist):
    return sum(sublist)

  

def parallel_reduce(func, data, chunks=4):
    chunk_size = len(data) // chunks
    chunks_list = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(chunks)]

  
    with Pool() as pool:
        partial_sums = pool.map(func, chunks_list)
    print(partial_sums)
    return reduce(lambda x,y: x + y, partial_sums)

  
if __name__=="__main__":
    total_sum = parallel_reduce(chunk_sum, numbers, chunks=4)
    print(total_sum)  # Виведе: 36
```

У цьому прикладі ми розбиваємо вхідний список на кілька частин, паралельно підраховуємо суму кожного блоку, а потім використовуємо `reduce` для остаточного об’єднання результатів.
### **4. Вступ до MapReduce**

Тепер, коли ми розібрали основи функціонального програмування та як `map`, `reduce` і `filter` працюють у контексті паралельних обчислень, настав час перейти до **MapReduce**. Це технологія, яка дозволяє обробляти великі обсяги даних у розподілених системах, ефективно використовуючи концепції, які ми вже розглянули.

#### **Що таке MapReduce?**

MapReduce — це програмна модель для паралельної обробки великих обсягів даних, що  використовується в розподілених системах. Вона заснована на двох основних операціях:

- **Map** — розбиття вхідних даних на незалежні частини, які можна обробляти паралельно.
- **Reduce** — агрегування результатів після обробки.

Ця модель є надзвичайно ефективною, оскільки дозволяє виконувати обчислення на тисячах вузлів одночасно. Вона активно використовується у великих системах, таких як **Hadoop**, **Apache Spark** і **Google BigQuery**.

#### **Як працює MapReduce?**

MapReduce складається з кількох ключових етапів:

1. **Фаза Map**  
    Вхідні дані розбиваються на частини (шматки) та обробляються незалежними екземплярами функції `map`. Кожен вузол виконує свою частину роботи, а вихідними даними є проміжні результати у вигляді пар **(ключ, значення)**. Наприклад, для задачі підрахунку слів у тексті результатом цього етапу будуть пари `("слово", 1)`, які вказують, що дане слово зустрілося один раз.
    
2. **Фаза Shuffle (Перемішування)**  
    Після того, як усі вузли виконали свою частину роботи, необхідно згрупувати однакові ключі разом. Цей процес називається "перемішуванням" (`shuffle`) і виконується автоматично системою. У результаті всі значення для одного ключа опиняються в одному місці.
    
3. **Фаза Reduce**  
    Після групування даних застосовується функція `reduce`, яка агрегує всі значення, пов’язані з одним ключем. У випадку підрахунку слів вона підсумовує всі одиниці, отримуючи загальну кількість появ кожного слова.
    

#### **Чому MapReduce працює добре у кластерних середовищах?**

Основна причина успішності MapReduce у тому, що воно мінімізує взаємодію між вузлами. Оскільки етап `map` не потребує координації між процесами, обчислення можуть відбуватися незалежно, що дозволяє ефективно масштабувати систему. Крім того, система може автоматично обробляти збої: якщо один з вузлів виходить з ладу, його завдання може бути перерозподілено на інший вузол.

Розглянемо загальну схему виконання MapReduce на реальному прикладі у наступному розділі.


### **5. Реалізація простого прикладу MapReduce 

Щоб краще зрозуміти, як працює MapReduce, розглянемо класичний приклад **підрахунку слів у тексті**. Це стандартна задача, яка ілюструє принцип роботи цієї моделі та використання основних етапів `Map`, `Shuffle` і `Reduce`.

#### **Опис задачі**

Припустимо, у нас є текстовий файл, у якому потрібно підрахувати кількість повторень кожного слова. Наприклад, якщо текст виглядає так:

```
hello world hello
```

то очікуваний результат після обробки повинен бути:

```
hello 2
world 1
```

#### **Реалізація MapReduce у Python без бібліотек**

Для початку реалізуємо цю задачу без використання спеціалізованих бібліотек, щоб зрозуміти, як працюють основні принципи.

##### **Етап 1: Фаза Map**

На цьому етапі ми розбиваємо текст на слова та створюємо проміжний список пар `(слово, 1)`.

```python
def map_function(text):
    words = text.split()
    return [(word, 1) for word in words]

text = "hello world hello"
mapped = map_function(text)

print(mapped)
# Виведе: [('hello', 1), ('world', 1), ('hello', 1)]
```

##### **Етап 2: Фаза Shuffle**

Після отримання проміжних пар необхідно згрупувати однакові ключі разом.

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(mapped)
print(shuffled)
# Виведе: {'hello': [1, 1], 'world': [1]}
```

##### **Етап 3: Фаза Reduce**

На цьому етапі ми обчислюємо фінальні значення для кожного ключа.

```python
def reduce_function(shuffled_data):
    return {key: sum(values) for key, values in shuffled_data.items()}

reduced = reduce_function(shuffled)
print(reduced)
# Виведе: {'hello': 2, 'world': 1}
```

Ця реалізація добре ілюструє принципи роботи MapReduce:

- **Функція `map_function`** розбиває текст на слова і додає `1` до кожного.
- **Функція `shuffle`** групує однакові ключі разом.
- **Функція `reduce_function`** підсумовує значення для кожного слова.

---

#### **Реалізація MapReduce з використанням multiprocessing**

Якщо текст дуже великий, його можна обробляти паралельно. Використаємо `multiprocessing` для розпаралелювання етапу `map`.

```python
from multiprocessing import Pool
from functools import reduce

text_data = [
    "hello world hello",
    "map reduce example",
    "reduce map example example"
]

def map_function(text):
    words = text.split()
    return [(word, 1) for word in words]

with Pool() as pool:
    mapped_results = pool.map(map_function, text_data)

# Об’єднуємо всі результати з різних частин тексту
flat_mapped = [pair for sublist in mapped_results for pair in sublist]

shuffled = shuffle(flat_mapped)
reduced = reduce_function(shuffled)

print(reduced)
# Виведе: {'hello': 2, 'world': 1, 'map': 2, 'reduce': 2, 'example': 3}
```

У цьому коді:

- `Pool.map()` розпаралелює обробку текстових фрагментів.
- `flat_mapped` об’єднує результати всіх процесів.
- `shuffle` та `reduce_function` виконують групування та підсумовування.

---


### Приклад примітивної LLM
Можна реалізувати **примітивну мовну модель (LLM)**, використовуючи концепції `map`, `reduce` та `filter` у стилі MapReduce. Найпростіший варіант — це **передбачення наступного слова на основі частотного аналізу**.

### **Ідея моделі**

1. **Вхідні дані** – великий корпус текстів.
2. **Фаза Map** – розбиваємо текст на біграми (пари слів) у вигляді `(попереднє_слово, наступне_слово) → 1`.
3. **Фаза Shuffle** – групуємо всі однакові біграми.
4. **Фаза Reduce** – підраховуємо частоти появи наступного слова для кожного попереднього.
5. **Прогнозування** – на основі частотного аналізу вибираємо найімовірніше наступне слово.

---

### **Реалізація MapReduce для простої мовної моделі**

Спершу підготуємо корпус тексту:

```python
text_data = [
    "the quick brown fox jumps over the lazy dog",
    "the quick brown fox is fast",
    "the fox is very quick and smart"
]
```

#### **Етап 1: Map – Створення біграм**

Розбиваємо текст на пари слів у вигляді `("the", "quick")`, `("quick", "brown")` тощо:

```python
def map_function(text):
    words = text.split()
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return [(bigram, 1) for bigram in bigrams]

mapped_results = []
for line in text_data:
    mapped_results.extend(map_function(line))

print(mapped_results)
# Виведе список пар: [(('the', 'quick'), 1), (('quick', 'brown'), 1), ...]
```

#### **Етап 2: Shuffle – Групування біграм**

Агрегуємо всі однакові пари разом:

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(mapped_results)
print(shuffled)
# Виведе: {('the', 'quick'): [1, 1], ('quick', 'brown'): [1, 1], ...}
```

#### **Етап 3: Reduce – Підрахунок частот**

Підсумовуємо кількість появ кожної біграми:

```python
def reduce_function(shuffled_data):
    return {key: sum(values) for key, values in shuffled_data.items()}

reduced = reduce_function(shuffled)
print(reduced)
# Виведе: {('the', 'quick'): 2, ('quick', 'brown'): 2, ('brown', 'fox'): 2, ...}
```

#### **Етап 4: Прогнозування наступного слова**

Створюємо функцію, яка на основі цього аналізу буде пропонувати найімовірніше слово.

```python
def predict_next_word(word, model):
    candidates = {key[1]: count for key, count in model.items() if key[0] == word}
    if not candidates:
        return None
    return max(candidates, key=candidates.get)  # Вибираємо слово з найбільшою частотою

print(predict_next_word("the", reduced))  # Очікуваний результат: "quick"
print(predict_next_word("quick", reduced))  # Очікуваний результат: "brown"
print(predict_next_word("fox", reduced))  # Очікуваний результат: "is" або "jumps"
```

---

Ця модель працює за дуже спрощеним принципом: вона аналізує частотність пар слів і прогнозує наступне слово, виходячи з історичних частот. У реальних великих LLM використовується набагато складніший підхід: наприклад, **n-грамні моделі**, **word embeddings** (як Word2Vec), **трансформери**, які враховують контекст набагато ширше.

Проте цей підхід наочно демонструє, що MapReduce можна застосовувати для мовних задач, і, використовуючи розподілені обчислення, можна ефективно працювати з дуже великими корпусами текстів.

Наступний крок — якщо хочемо масштабувати це на великі набори даних, можна запаралелити обробку тексту за допомогою `multiprocessing`, як ми робили раніше з `map`.


Реалізуємо **розподілену обробку корпусу текстів**, уявивши, що у нас є **чотири книги у вигляді текстових файлів**. Використаємо **multiprocessing** для обробки даних у паралельному режимі.

---

### **Ідея підходу**

1. **Читаємо текстові файли паралельно**  
    Розбиваємо кожну книгу на біграми (пари слів).
2. **Фаза Map (паралельна)**  
    Використовуємо `Pool.map()`, щоб кожен процес обробляв свою частину тексту.
3. **Фаза Shuffle**  
    Групуємо біграми, щоб всі однакові ключі виявилися в одному місці.
4. **Фаза Reduce (паралельна)**  
    Використовуємо `Pool.map()`, щоб кожен процес підсумовував частоти біграм.
5. **Прогнозування наступного слова**  
    Використовуємо частотний аналіз для вибору найбільш імовірного наступного слова.

---

### **Реалізація паралельної обробки корпусу текстів**

#### **1. Функція читання тексту**

Читаємо вміст кожного файлу:

```python
def read_file(filename):
    with open(filename, "r") as f:
        return f.read()
```

---

#### **2. Паралельна фаза Map**

Кожен процес бере свій шматок тексту, розбиває його на слова та створює біграми:

```python
def map_function(text):
    words = text.lower().split()  # Зменшуємо регістр і розбиваємо на слова
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return [(bigram, 1) for bigram in bigrams]
```

Обробимо всі файли **паралельно**:

```python
from multiprocessing import Pool

file_list = [f"book_{i+1}.txt" for i in range(4)]  # 4 книги

with Pool() as pool:
    texts = pool.map(read_file, file_list)  # Читаємо всі книги
    mapped_results = pool.map(map_function, texts)  # Паралельна обробка тексту
```

Об’єднуємо всі біграми з різних процесів в один список:

```python
flat_mapped = [pair for sublist in mapped_results for pair in sublist]
```

---

#### **3. Фаза Shuffle**

Групуємо всі біграми разом:

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(flat_mapped)
```

---

#### **4. Паралельна фаза Reduce**

Тепер запустимо зведення частот **паралельно**:

```python
def reduce_function(key_values):
    key, values = key_values
    return (key, sum(values))

with Pool() as pool:
    reduced_results = pool.map(reduce_function, shuffled.items())

# Перетворюємо список у словник
final_model = dict(reduced_results)
```

---

#### **5. Прогнозування наступного слова**

Створюємо функцію, яка пропонує найбільш вживане наступне слово:

```python
def predict_next_word(word, model):
    candidates = {key[1]: count for key, count in model.items() if key[0] == word}
    if not candidates:
        return None
    return max(candidates, key=candidates.get)  # Найчастіше вживане слово

# Тестуємо прогнозування
print(predict_next_word("the", final_model))  # Очікуваний результат: "fox" або "journey"
print(predict_next_word("knowledge", final_model))  # Очікуваний результат: "is"
print(predict_next_word("to", final_model))  # Очікуваний результат: "be"
```

---

### **Висновки**

1. Ми створили **паралельну реалізацію MapReduce** для побудови **мовної моделі**, яка аналізує корпус текстів та передбачає наступне слово на основі частотного аналізу.
2. **Map (паралельний)** — кожен процес обробляє свою частину тексту, створюючи біграми.
3. **Shuffle (групування)** — всі однакові біграми об’єднуються.
4. **Reduce (паралельний)** — частоти біграм підсумовуються у декількох процесах.
5. **Прогнозування** — використовуємо найчастішу біграму для вибору наступного слова.

Цей підхід можна масштабувати на **великі текстові корпуси** та **кластерні системи**, де окремі вузли виконують `map`, а потім об’єднують результати через `reduce`.

фінальна версія коду **одним файлом**, яка реалізує **паралельну обробку текстових файлів** та **просту мовну модель на основі біграм** за допомогою `multiprocessing`.
https://github.com/VolodymyrBodnar/n-grams/blob/main/bi_gram.py

## Приклад з n-грамами та вікіпедією
```
pip install wikipedia-api
```

https://github.com/VolodymyrBodnar/n-grams/blob/main/n-gram.py


## Приклад з GPT
```bash
pip install transformers torch
```


  

### **Теоретичне обґрунтування нашої роботи: Шлях від статистики до розуміння**


Нашою кінцевою метою було створення системи, яка могла б осмислено відповідати на запитання, спираючись на наданий їй набір текстових документів. Ми пройшли кілька етапів еволюції, кожен з яких вирішував проблеми попереднього.

---
### **Етп 1: Початкова ідея — Статистичне Прогнозування (N-грамні моделі)**
  
На самому початку ми спробували реалізувати найпростішу мовну модель, засновану на **N-грамах**.
  
**Теоретична суть:**  
N-грамна модель базується на "гіпотезі Маркова", яка припускає, що наступне слово в послідовності залежить лише від кількох попередніх (N-1) слів. Це чисто статистичний підхід. Ми не намагаємося "зрозуміти" текст, а лише підраховуємо, яке слово найчастіше з'являється після певної комбінації слів.
  
**Приклади:**  
Біграма (n=2): Яке слово найчастіше йде після слова "функціональне"?  
Триграма (n=3): Яке слово найчастіше йде після слів "функціональне програмування"?

**Зв'язок з MapReduce:**  
Ця задача ідеально лягає на парадигму MapReduce.  
- **Фаза `Map`:** Процес-мапер проходить по тексту і генерує пари `( (попередні_слова), наступне_слово )` з лічильником `1`.  
- **Фаза `Reduce`:** Процес-редуктор збирає всі однакові пари і сумує їхні лічильники, отримуючи фінальну частоту.

  

**Обмеження:**  
Такий підхід не враховує довгостроковий контекст, граматику чи сенс речення. Це просто "папуга", що повторює найчастіші послідовності.
  

--- 
### **Етап 2: Перший прорив — Додавання пам'яті та контексту (RAG)**

Щоб подолати обмеження статистики, ми впровадили значно складнішу і потужнішу архітектуру — **Retrieval-Augmented Generation (RAG)**.

**Теоретична суть:**  
RAG розділяє процес відповіді на два етапи:  
1. **Retrieval (Пошук):** Пошук релевантної інформації.  
2. **Generation (Генерація):** Формування відповіді на основі знайденого.

  

**Як ми реалізували Retrieval:**  
- **Чанкінг (Chunking):** Розбиття корпусу на семантично зв'язані фрагменти.  
- **Векторизація (Vectorization):** Перетворення текстів на вектори через **TF-IDF**, що оцінює вагу слів.  
- **Пошук подібності (Similarity Search):** Використання **косинусної подібності** для пошуку схожих за сенсом текстів.

**Проблема:**  
Retriever працював добре, але генератор залишався N-грамним, що обмежувало "розуміння".

---
### **Етап 3: Сучасне рішення — Інтеграція Попередньо Навченої LLM**
Ми замінили N-грамний генератор на **велику мовну модель (LLM)**.

**Теоретична суть:**  
LLM — це трансформерна нейронна мережа, навчена на гігантських масивах текстів. Вона володіє граматикою, стилістикою, знаннями та здатністю до умовного міркування.
  
**Ключові уроки:**  
1. **Model Selection:** Неправильно підібрана модель може не працювати для конкретного завдання.  
2. **Prompt Engineering:** Модель потребує чітких інструкцій. Використання правильного формату запиту (мови, структури) кардинально змінює результат.

---


### **Висновки: Що ми збудували в підсумку**

Наша фінальна архітектура — це **RAG-система**, промисловий стандарт для чат-ботів та QA-систем.

Принцип роботи:  
1. Отримує запит.  
2. Звертається до своєї бази знань.  
3. Виконує **Retrieval**.  
4. Формує промпт із знайденими фактами.  
5. Використовує LLM для формування відповіді.

Ми пройшли шлях від простого підрахунку слів до створення багатокомпонентної системи, що ілюструє ключові концепції сучасної науки про дані.


## Бонус

## Теоретичний огляд: Архітектура Retrieval-Augmented Generation (RAG)

**Retrieval-Augmented Generation (RAG)** — це архітектура штучного інтелекту, що поєднує потужності попередньо навчених великих мовних моделей (LLM) із можливістю доступу до зовнішніх баз знань. Основна мета RAG — подолати фундаментальні обмеження стандартних LLM, такі як "галюцинації" (вигадування фактів), нездатність використовувати актуальну або приватну інформацію та відсутність прозорості у джерелах відповідей.

Архітектура RAG динамічно надає моделі релевантну інформацію "на льоту", що дозволяє їй генерувати відповіді, які є не тільки стилістично правильними, але й фактично обґрунтованими та контекстуально доречними.

---
### Компоненти архітектури RAG
Система RAG складається з двох основних, послідовно працюючих компонентів: **Пошуковця (Retriever)** та **Генератора (Generator)**.
#### 1. Пошуковець (The Retriever) 
Завдання Пошуковця — знайти й витягнути найбільш релевантну інформацію з корпусу документів у відповідь на запит користувача. Цей процес складається з двох етапів:
- **Підготовка бази знань (Індексація):** Цей етап виконується офлайн.
    1. **Завантаження та сегментація (Chunking):** Документи з бази знань (статті, книги, файли) завантажуються і розбиваються на менші, логічно завершені фрагменти або "чанки". Правильний розмір чанків є критично важливим для ефективності пошуку.
    2. **Вбудовування (Embedding):** Кожен чанк тексту перетворюється на числовий вектор (embedding) за допомогою спеціальної моделі-енкодера (наприклад, Sentence-BERT або інші трансформери). Ці вектори фіксують семантичне значення тексту і зберігаються у спеціалізованій **векторній базі даних**. Ця база даних оптимізована для швидкого пошуку подібності між векторами.
- **Процес пошуку (Retrieval):** Цей етап виконується в реальному часі.
    1. Коли користувач надсилає запит, його текст також перетворюється на вектор за допомогою тієї ж моделі-енкодера.
    2. Система виконує пошук у векторній базі даних, знаходячи вектори чанків, які є "найближчими" до вектора запиту (зазвичай за допомогою **косинусної подібності**).
    3. N найбільш релевантних чанків витягуються і передаються на наступний етап.
#### 2. Генератор (The Generator) 
Завдання Генератора — синтезувати фінальну, когерентну відповідь для користувача, використовуючи як оригінальний запит, так і отриманий від Пошуковця контекст.
1. **Промпт-інженерія (Prompt Engineering):** Система не просто передає запит до LLM. Вона формує розширений, структурований **промпт**, який містить шаблон-інструкцію, витягнуті чанки (контекст) та оригінальне питання користувача.
2. **Доповнена генерація (Augmented Generation):** Сформований промпт подається у велику мовну модель (наприклад, GPT, Llama). LLM використовує наданий контекст як "шпаргалку" або джерело істини, генеруючи відповідь, що спирається на ці факти.
---
### Переваги архітектури RAG
- **Зменшення "галюцинацій":** Оскільки модель генерує відповідь на основі конкретного тексту, ризик вигадування фактів значно знижується.
- **Доступ до актуальної та приватної інформації:** RAG дозволяє LLM використовувати дані, яких не було в її навчальній вибірці, наприклад, новини за останню годину або внутрішні документи компанії.
- **Прозорість і можливість цитування:** Система може вказати, на основі яких саме фрагментів тексту було згенеровано відповідь, що підвищує довіру до неї.
- **Ефективність:** Оновити знання системи значно простіше — достатньо оновити векторну базу даних, що набагато дешевше і швидше, ніж перенавчати або донавчати багатомільярдну LLM

---

### **Теоретичний аналіз: Проблема репетитивної генерації в авторегресійних мовних моделях**

Одним із поширених і добре задокументованих недоліків авторегресійних мовних моделей, особливо моделей меншого розміру або при певних конфігураціях, є тенденція до **репетитивної генерації** (зациклювання). Цей феномен проявляється у повторенні одного й того ж слова, фрази або речення, що призводить до семантично збідненого та некогерентного тексту. Хоча зовні це може нагадувати обмеження N-грамних моделей, фундаментальна причина є значно складнішою і пов'язана зі стратегією вибору наступного токена.

---

#### **Механізм генерації та стратегії декодування**

Процес генерації тексту в мовній моделі є покроковим (авторегресійним). На кожному кроці модель, аналізуючи всю попередню згенеровану послідовність, обчислює **розподіл ймовірностей** по всьому своєму словнику для наступного токена. Після цього необхідно застосувати **стратегію декодування (decoding strategy)**, щоб обрати один токен із цього розподілу.

Основною причиною зациклювання є застосування найпростішої стратегії — **жадібного пошуку (greedy search)**. Ця стратегія полягає в детермінованому виборі токена з максимальною ймовірністю на кожному кроці. Такий підхід може призвести до потрапляння моделі в "пастку" зворотного зв'язку:

1. Модель генерує токен `T1`, оскільки він мав найвищу ймовірність.
2. На наступному кроці, аналізуючи послідовність, що тепер закінчується на `T1`, модель може знову визначити `T1` як найімовірніший наступний токен.
3. Оскільки стратегія жадібного пошуку завжди обирає максимум, модель знову генерує `T1`, замикаючи цикл.

Це явище особливо помітне, коли модель стикається з контекстом, який не надає достатньо інформації для різноманітного продовження, або коли внутрішні патерни моделі є недостатньо складними.

---

#### **Методи боротьби з репетитивною генерацією**

Для усунення цієї проблеми було розроблено низку більш просунутих технік декодування, які вводять елемент стохастичності або застосовують регуляризацію.

- **Температура (Temperature Sampling):** Цей параметр змінює форму розподілу ймовірностей. Значення температури `T > 1` "згладжує" розподіл, збільшуючи ймовірність менш очікуваних токенів і тим самим сприяючи різноманітності. Значення `T < 1` робить розподіл більш "гострим", посилюючи ймовірність найочевидніших токенів.
- **Вибірка з K найкращих (Top-K Sampling):** Замість того, щоб розглядати весь словник, на кожному кроці відбирається `K` найімовірніших токенів, і вибір здійснюється тільки з цього обмеженого набору. Це відсікає "хвіст" малоймовірних і часто недоречних слів.
- **Ядерна вибірка (Top-p / Nucleus Sampling):** Більш адаптивний підхід, ніж Top-K. Замість фіксованої кількості `K` токенів, обирається найменший набір токенів, сукупна ймовірність яких перевищує поріг `p`. Таким чином, кількість кандидатів динамічно змінюється залежно від "впевненості" моделі.
- **Штраф за повторення (Repetition Penalty):** Це прямий метод боротьби з циклами. Параметр `repetition_penalty > 1.0` штучно знижує ймовірність тих токенів, які вже з'являлися в згенерованому тексті. Це змушує модель шукати альтернативні варіанти і уникати повторень.

Застосування цих технік, окремо або в комбінації, дозволяє значно покращити якість і когерентність згенерованого тексту, усуваючи артефакти, пов'язані з простими стратегіями декодування.

```python
generated_text = generator(
            prompt,
            max_length=200,
            num_return_sequences=1,
            # Додаємо штраф за повторення
            repetition_penalty=1.2,
            # Також можемо додати top_k, щоб обмежити вибір слів
            top_k=50
        )[0]['generated_text']
```

# Локальний запуск LLM DeepSeek для RAG-системи

Для отримання якісних відповідей від нашої RAG-системи, особливо українською мовою, ми перейшли від хмарних, загальних моделей до локального запуску потужної моделі **DeepSeek**. Цей підхід забезпечує максимальну якість, приватність даних та повний контроль над процесом генерації.

---
### ## 1. Технічні вимоги (Що потрібно мати)

Локальний запуск є вимогливим до комп'ютера, тому перед початком переконайтеся, що ваша система відповідає наступним критеріям:
- **Оперативна пам'ять (RAM):** Мінімум **16 ГБ**.
- **Відеокарта (GPU):** Рекомендовано **NVIDIA** з **8 ГБ+ VRAM** (наприклад, RTX 3060 12 ГБ є ідеальним варіантом). Без GPU запуск можливий, але буде дуже повільним.
- **Місце на диску:** Близько **5-10 ГБ** для файлу самої моделі.

---

### ## 2. Встановлення необхідних бібліотек

Для ефективної роботи з моделлю ми використовуємо спеціалізовану бібліотеку `llama-cpp-python`.

- Якщо у вас є відеокарта NVIDIA (рекомендовано):

Відкрийте термінал (командний рядок) і виконайте команду, яка встановить бібліотеку з підтримкою GPU:
```Bash
   CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
```

- **Якщо у вас немає підходящої відеокарти (лише CPU):**
 ```
  pip install llama-cpp-python
```    

---

### ## 3. Завантаження моделі

Нам потрібна спеціальна, оптимізована версія моделі у форматі **GGUF**.

1. **Перейдіть на Hugging Face:** Відкрийте сторінку [TheBloke/deepseek-coder-6.7b-instruct-GGUF](https://huggingface.co/TheBloke/deepseek-coder-6.7b-instruct-GGUF).
2. **Завантажте файл:** У вкладці "Files and versions" знайдіть і завантажте файл `deepseek-coder-6.7b-instruct.Q4_K_M.gguf`. Це хороший баланс між розміром та якістю.
3. **Збережіть:** Покладіть завантажений файл у папку з вашим Python-скриптом.

---
### ## 4. Зміни в коді
Тепер необхідно інтегрувати локальну модель у наш проєкт.
#### **Крок 1: Імпортуйте `Llama`**
На початку файлу додайте новий імпорт:

```Python
from llama_cpp import Llama
```
#### **Крок 2: Замініть ініціалізацію моделі**

Знайдіть і замініть блок, де ви раніше викликали `pipeline`, на новий код, що завантажує локальну модель:

```Python
# Замість pipeline('text-generation', ...)
print("🚀 Завантажуємо локальну модель DeepSeek (GGUF)...")
llm = Llama(
  model_path="./deepseek-coder-6.7b-instruct.Q4_K_M.gguf", # Назва вашого файлу
  n_ctx=4096,      # Розмір контексту
  n_gpu_layers=-1  # -1 для максимального використання GPU, 0 для CPU
)
print("✅ Модель успішно завантажена!")
```

- **Важливо:** Переконайтеся, що `model_path` вказує на правильну назву завантаженого файлу, а `n_gpu_layers` встановлено відповідно до вашої системи.

#### **Крок 3: Замініть виклик генератора**
У циклі `while True` замініть старий виклик `generator()` на новий, що працює з об'єктом `Llama`:

```Python
# Замість generated_text = generator(...)
print("⏳ Генерую відповідь...")
output = llm(
    prompt,
    max_tokens=512,
    stop=["Запитання:", "Контекст:", "Завдання:"],
    echo=False,
    temperature=0.7,
)
answer = output['choices'][0]['text'].strip()
print(f"\n🤖 Відповідь DeepSeek:\n{answer}")
```
Ці зміни дозволять вашій RAG-системі використовувати потужність локальної моделі DeepSeek для генерації значно якісніших та релевантніших відповідей.